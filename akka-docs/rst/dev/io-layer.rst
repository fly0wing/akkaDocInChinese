.. _io-layer:

################
I/O 层设计
################

``akka.io`` 包是 Akka
和 `spray.io`_ 团队合作开发的。它的设计合并了 ``spray-io`` 模块的经验，以及为了使之能作为一个基于actor的服务而更加通用地被消费，而一起开发的改进。

需求
============

为了形成一个针对大范围的应用程序的通用而可扩展的IO层基础，以Akka remoting和spray HTTP作为最初版本，下面的需求是这一需求的关键驱动力:

* 扩展到数以百万计的并发连接。

* 将数据从输入通道获取到目标actor的信箱需要尽可能小的时延。

* 最大的吞吐量

* 可选的双向背压 (也就是，节流本地发送者的同时，也允许本地读取者节流远程发送者，这是被协议允许的。)

* 纯粹基于actor的API，带有不可变的数据表示

* 通过一个非常精简的SPI用于集成新的传输方式，从而实现可扩展性;目标是不将I/O机制限制为一个最小公分母，而是允许完全特定于协议的用户级API。

基本架构
==================

每种传输实现将会作为一个单独的Akka扩展而使用，提供一个  :class:`ActorRef` 来表示客户代码的初始接入点。  "管理者" 接受建立信道的请求（例如，连接或监听一个TCP socket）。每个信道都是由一个专门的actor表示的，它在整个一生中被暴露给所有与这个通道交互的客户代码。

这个实现中的核心元素是特定于传输的 “选择器” actor; 在TCP的情况下，这会包装一个 :class:`java.nio.channels.Selector` 。这个通道actor注册它们对于可读性和可写性的兴趣的方式是，向它们所被分配的选择器actor发送响应的消息。然而，实际的通道读写操作是由通道actor自身来进行的，这将选择器actor从耗时较多的任务中解放出来，这样就确保了低延时。选择器actor仅有的职责就是管理底层selector的key集合，以及实际的select操作，这是唯一通常会阻塞的操作。
向selector分配通道的操作是由manager actor来执行的，并且在一个通道整个的一生中保持不变。通过这种方式，管理actor基于一些特定于实现的分发逻辑，在一个或更多的selector actor之间交替分配新的actor。这种逻辑可以被（部分）选择器 actor，它可以，在认为自身容量已满的情况下选择拒绝新通道的分配。

管理者actor创建（因此监管）了选择器actor，它转而创建并监控了自己的通道actor。一个单独的传输实现中的actor层次结构因此而包含三个不同的actor等级，顶层是管理actor，叶子节点是通道actor，而选择器actor位于中间层。

输出的背压的实现方式是，用户在其 :class:`Write`消息中指定是否想要接收写入O/S内核的入队应答。输入背压的实现方式是，向通道actor发送一条消息，临时为禁用通道读数据的兴趣，直到使用相对应的resume命令来重新启用读取功能。在使用流量控制的传输中-类似于TCP-在接收端未消费数据（于是导致它们保持在内核读取缓存中）的行为会被传播会发送者， 跨越网络可将这两种机制连接起来。

设计的好处
===============

将整个实现都保持在actor模型内部，允许我们去掉对显式线程处理逻辑的需求，并且它还意味着不涉及锁（除了作为底层传输库一部分的那些锁）。仅编写actor代码带来了更加清晰的实现，然而Akka高效的actor消息通讯并没有在这个好处之上添加高额的代价。实际上I/O的事件驱动本质如此好地映射到了actor模型，以至于相对于使用显式线程管理和同步的传统解决方案，我们更加期盼显著的性能，特别时可伸缩性的好处，

监管层次结构的另一个益处是，资源清理变得很自然：关闭一个选择器actor会自动清理所有的通道actor，允许对通道的正确关闭，并且向用户级的客户actor发送恰当的消息。 DeathWatch 允许通道actor注意它们的用户级处理器actor的死去，并且在那种情况下也以一种有序的方式来终止;这很自然地降低了泄露打开通道的几率。

如何着手添加一个新的传输方式
======================================

最好的起点是学习TCP参考实现，从而得到对基本工作原则的很好掌握，然后设计一个实现，其精神是类似的，但要适配到要使用的新协议。不同的I/O机制之间有显著的差异（例如，比较一下文件I/O和消息中介)，而此I/O层的目的显然 **不** 是将所有机制硬塞到一个统一的API中，这就是为何此处只说明基本架构观点。

.. _spray.io: http://spray.io

